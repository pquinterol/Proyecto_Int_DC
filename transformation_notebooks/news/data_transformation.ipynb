{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 30\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n%additional_python_modules polygon-api-client, nltk, transformers, beautifulsoup4, termcolor\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom urllib.request import urlopen, Request\nfrom bs4 import BeautifulSoup\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\nfrom polygon import RESTClient\nfrom polygon.rest.models import *\n\nfrom termcolor import colored as cl\nimport requests\nimport datetime\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.3 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 30 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 2\nAdditional python modules to be included:\npolygon-api-client\nnltk\ntransformers\nbeautifulsoup4\ntermcolor\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::973411499138:role/LabRole\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: 7915be7c-4cb0-4d72-9691-1c496dda84e3\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.3\n--enable-glue-datacatalog true\n--additional-python-modules polygon-api-client,nltk,transformers,beautifulsoup4,termcolor\nWaiting for session 7915be7c-4cb0-4d72-9691-1c496dda84e3 to get into ready status...\nSession 7915be7c-4cb0-4d72-9691-1c496dda84e3 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "def get_data_from_catalog(db_name, table, glue_context):\n    dyf = glueContext.create_dynamic_frame.from_catalog(database=db_name, table_name=table)\n    pandas_df = dyf.toDF().toPandas()\n    \n    return pandas_df\n    ",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import pandas as pd\nfrom awsglue.dynamicframe import DynamicFrame\n\ndef write_aws_table(\n    pandas_df, \n    s3_path, \n    glue_context, \n    spark_session, \n    db_name, \n    table_name\n):\n    s3output = glue_context.getSink(\n      path=s3_path,\n      connection_type=\"s3\",\n      updateBehavior=\"UPDATE_IN_DATABASE\",\n      partitionKeys=[],\n      compression=\"snappy\",\n      enableUpdateCatalog=True,\n      transformation_ctx=\"s3output\",\n    )\n    s3output.setCatalogInfo(\n      catalogDatabase=db_name, catalogTableName=table_name\n    )\n    s3output.setFormat(\"glueparquet\")\n    data_spark_df = spark.createDataFrame(pandas_df)\n    data_dyf = DynamicFrame.fromDF(data_spark_df, glue_context, f\"{table_name}_df\")\n    s3output.writeFrame(data_dyf)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "get_data_from_catalog(\"project\", \"kaggle\", glueContext)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "         index_key  ... ticker\n0              0.0  ...      A\n1              1.0  ...      A\n2              2.0  ...      A\n3              3.0  ...      A\n4              4.0  ...      A\n...            ...  ...    ...\n1400464  1413844.0  ...     ZX\n1400465  1413845.0  ...     ZX\n1400466  1413846.0  ...     ZX\n1400467  1413847.0  ...     ZX\n1400468  1413848.0  ...     ZX\n\n[1400469 rows x 4 columns]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "news_tables = [\"raw_polygon_news\", \"raw_wfc_2021_news\", \"raw_tgt_2021_news\", \"raw_tgt_2020_news\", \"raw_wfc_2020_news\", \"kaggle\"]\ndb = \"project\"\ncombined_news_data = pd.DataFrame()\ntickers = [\"TGT\", \"WFC\"]\n\nfor news_table in news_tables:\n    news_table_data = get_data_from_catalog(db, news_table, glueContext)\n    if news_table == \"kaggle\":\n        news_table_data[\"date\"] = pd.to_datetime(news_table_data[\"date\"], errors='coerce', format='%Y-%m-%d %H:%M:%S', utc= True)\n        news_table_data[\"date\"] = news_table_data[\"date\"].dt.strftime('%Y/%m/%d')\n        news_table_data = news_table_data.drop(\"index_key\", axis= 1)\n        news_table_data = news_table_data.loc[(news_table_data[\"date\"] > \"2019/01/01\") & (news_table_data[\"ticker\"].isin(tickers))]\n        \n    combined_news_data = pd.concat([combined_news_data, news_table_data])\n    \ncombined_news_data_target_fields = combined_news_data[[\"title\", \"date\", \"ticker\"]]\ncombined_news_data_target_fields",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "                                                     title        date ticker\n0            Top Ranked Growth Stocks to Buy for June 16th  2021/06/16    TGT\n1                        Wall Street Breakfast: Taper Talk  2021/08/19    TGT\n2        Target shares continue declining to find poten...  2022/02/26    TGT\n3            Top Ranked Growth Stocks to Buy for June 23rd  2021/06/23    TGT\n4        Private Brands Alone Can't Save Bed Bath & Beyond  2021/06/06    TGT\n...                                                    ...         ...    ...\n1337347  UPDATE: Citi Believes 'risk/reward for the ban...  2019/01/10    WFC\n1337348  This Day In Market History: First American Ban...  2019/01/07    WFC\n1337349  Wells Fargo's Valuation Discounts Regulatory C...  2019/01/02    WFC\n1337350  Benzinga's Top Upgrades, Downgrades For Januar...  2019/01/02    WFC\n1337351  RBC Capital Upgrades Wells Fargo to Sector Per...  2019/01/02    WFC\n\n[7471 rows x 3 columns]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Guardado en Transformed",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "write_aws_table(\n    combined_news_data_target_fields, \n    \"s3://project-2023-datalake/transformed/tickers_news/wfc_tgt_news_2019_to_2023/\", \n    glueContext, \n    spark, \n    \"project\", \n    \"transformed_wfc_tgt_news\"\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Checkeo de fechas faltantes",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def get_missing_dates(values, init_date, end_date):\n  values = set(values)\n  missing_dates = []\n  init_date_dt = datetime.datetime.strptime(init_date, '%Y/%m/%d')\n  end_date_dt = datetime.datetime.strptime(end_date, '%Y/%m/%d')\n  date_diff = (end_date_dt - init_date_dt).days\n\n  for day_index in range(1, date_diff + 1):\n    curr_date = init_date_dt + datetime.timedelta(days = day_index)\n    curr_date_str = curr_date.strftime('%Y/%m/%d')\n    #print(f\"Current date in datetime: {curr_date}\")\n\n    if init_date_dt <= curr_date <= end_date_dt and curr_date_str in values:\n      #print(f\"{curr_date_str} date not missing\")\n      continue\n    else:\n      missing_dates.append(curr_date_str)\n      #print(f\"Missing date: {curr_date_str}\")\n  \n  return missing_dates",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Missing date in overall dataset\nINIT_DATE = '2019/01/01'\nEND_DATE = '2023/05/28'\n\nnews_date_vals = pd.unique(combined_news_data_target_fields[\"date\"])\nnews_missing_dates = get_missing_dates(news_date_vals, INIT_DATE, END_DATE)\nprint(f\" Missing dates between {INIT_DATE} - {END_DATE}:\")\nprint(news_missing_dates)\nprint(f\"Total count: {len(news_missing_dates)}\")\nprint(f\"Fraction of a Year missing days: {len(news_missing_dates)/365}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": " Missing dates between 2019/01/01 - 2023/05/28:\n['2019/01/03', '2019/01/04', '2019/01/05', '2019/01/06', '2019/01/12', '2019/01/17', '2019/01/18', '2019/01/19', '2019/01/20', '2019/01/21', '2019/01/23', '2019/01/25', '2019/01/26', '2019/01/27', '2019/01/30', '2019/01/31', '2019/02/03', '2019/02/05', '2019/02/06', '2019/02/09', '2019/02/10', '2019/02/11', '2019/02/14', '2019/02/16', '2019/02/18', '2019/02/21', '2019/02/22', '2019/02/25', '2019/02/26', '2019/03/02', '2019/03/03', '2019/03/07', '2019/03/08', '2019/03/09', '2019/03/10', '2019/03/11', '2019/03/14', '2019/03/15', '2019/03/16', '2019/03/17', '2019/03/19', '2019/03/23', '2019/03/24', '2019/03/27', '2019/03/31', '2019/04/03', '2019/04/05', '2019/04/06', '2019/04/10', '2019/04/13', '2019/04/14', '2019/04/19', '2019/04/20', '2019/04/21', '2019/04/22', '2019/04/23', '2019/04/25', '2019/04/27', '2019/04/30', '2019/05/01', '2019/05/05', '2019/05/07', '2019/05/09', '2019/05/11', '2019/05/12', '2019/05/18', '2019/05/19', '2019/05/26', '2019/05/27', '2019/05/30', '2019/06/01', '2019/06/02', '2019/06/03', '2019/06/06', '2019/06/07', '2019/06/08', '2019/06/09', '2019/06/14', '2019/06/16', '2019/06/19', '2019/06/20', '2019/06/21', '2019/06/22', '2019/06/23', '2019/06/24', '2019/06/25', '2019/06/29', '2019/07/03', '2019/07/04', '2019/07/05', '2019/07/06', '2019/07/07', '2019/07/13', '2019/07/14', '2019/07/21', '2019/07/22', '2019/07/25', '2019/07/26', '2019/07/27', '2019/07/28', '2019/07/29', '2019/08/03', '2019/08/04', '2019/08/06', '2019/08/08', '2019/08/09', '2019/08/10', '2019/08/11', '2019/08/18', '2019/08/27', '2019/08/30', '2019/08/31', '2019/09/01', '2019/09/02', '2019/09/03', '2019/09/04', '2019/09/07', '2019/09/08', '2019/09/11', '2019/09/14', '2019/09/15', '2019/09/16', '2019/09/18', '2019/09/21', '2019/09/22', '2019/09/25', '2019/09/26', '2019/10/03', '2019/10/05', '2019/10/06', '2019/10/07', '2019/10/12', '2019/10/13', '2019/10/19', '2019/10/20', '2019/10/24', '2019/10/27', '2019/11/01', '2019/11/02', '2019/11/03', '2019/11/05', '2019/11/09', '2019/11/16', '2019/11/17', '2019/11/24', '2019/11/25', '2019/11/28', '2019/11/30', '2019/12/01', '2019/12/05', '2019/12/07', '2019/12/08', '2019/12/13', '2019/12/14', '2019/12/15', '2019/12/21', '2019/12/22', '2019/12/25', '2019/12/28', '2019/12/29', '2020/01/01', '2020/01/04', '2020/01/05', '2020/01/07', '2020/01/12', '2020/01/18', '2020/01/19', '2020/01/20', '2020/01/22', '2020/01/25', '2020/01/26', '2020/02/01', '2020/02/02', '2020/02/03', '2020/02/06', '2020/02/07', '2020/02/08', '2020/02/09', '2020/02/15', '2020/02/16', '2020/02/17', '2020/02/19', '2020/02/23', '2020/02/26', '2020/02/29', '2020/03/01', '2020/03/08', '2020/03/14', '2020/03/22', '2020/03/29', '2020/04/04', '2020/04/05', '2020/04/11', '2020/04/12', '2020/04/19', '2020/04/20', '2020/04/25', '2020/04/26', '2020/05/02', '2020/05/03', '2020/05/10', '2020/05/17', '2020/05/23', '2020/05/24', '2020/05/25', '2020/05/28', '2020/05/30', '2020/06/06', '2020/06/07', '2020/06/13', '2020/06/14', '2020/06/16', '2020/06/20', '2020/06/22', '2020/06/27', '2020/06/28', '2020/06/30', '2020/07/03', '2020/07/04', '2020/07/05', '2020/07/12', '2020/07/17', '2020/07/19', '2020/07/24', '2020/07/25', '2020/07/26', '2020/07/31', '2020/08/01', '2020/08/02', '2020/08/04', '2020/08/05', '2020/08/08', '2020/08/09', '2020/08/11', '2020/08/12', '2020/08/13', '2020/08/15', '2020/08/16', '2020/08/17', '2020/08/21', '2020/08/23', '2020/08/25', '2020/08/27', '2020/08/28', '2020/08/29', '2020/08/30', '2020/08/31', '2020/09/01', '2020/09/02', '2020/09/03', '2020/09/04', '2020/09/06', '2020/09/07', '2020/09/08', '2020/09/10', '2020/09/11', '2020/09/12', '2020/09/13', '2020/09/14', '2020/09/16', '2020/09/17', '2020/09/18', '2020/09/19', '2020/09/20', '2020/09/21', '2020/09/22', '2020/09/23', '2020/09/24', '2020/09/25', '2020/09/26', '2020/09/27', '2020/10/01', '2020/10/02', '2020/10/03', '2020/10/04', '2020/10/05', '2020/10/06', '2020/10/07', '2020/10/08', '2020/10/09', '2020/10/10', '2020/10/11', '2020/10/17', '2020/10/18', '2020/10/19', '2020/10/21', '2020/10/22', '2020/10/23', '2020/10/24', '2020/10/25', '2020/10/26', '2020/10/27', '2020/10/28', '2020/10/29', '2020/10/30', '2020/10/31', '2020/11/01', '2020/11/02', '2020/11/04', '2020/11/08', '2020/11/10', '2020/11/11', '2020/11/13', '2020/11/14', '2020/11/15', '2020/11/16', '2020/11/20', '2020/11/21', '2020/11/22', '2020/11/25', '2020/11/26', '2020/11/28', '2020/11/29', '2020/12/05', '2020/12/09', '2020/12/10', '2020/12/11', '2020/12/13', '2020/12/17', '2020/12/18', '2020/12/19', '2020/12/24', '2020/12/25', '2020/12/27', '2020/12/28', '2021/01/01', '2021/01/02', '2021/01/03', '2021/01/04', '2021/01/05', '2021/01/06', '2021/01/07', '2021/01/08', '2021/01/09', '2021/01/10', '2021/01/11', '2021/01/14', '2021/01/15', '2021/01/17', '2021/01/18', '2021/01/19', '2021/01/21', '2021/01/22', '2021/01/23', '2021/01/24', '2021/01/25', '2021/01/26', '2021/01/27', '2021/01/28', '2021/01/29', '2021/01/30', '2021/01/31', '2021/02/01', '2021/02/02', '2021/02/03', '2021/02/04', '2021/02/05', '2021/02/06', '2021/02/07', '2021/02/08', '2021/02/13', '2021/02/14', '2021/02/16', '2021/02/19', '2021/02/21', '2021/02/28', '2021/03/05', '2021/03/06', '2021/03/07', '2021/03/09', '2021/03/19', '2021/03/28', '2021/04/03', '2021/04/06', '2021/04/20', '2021/05/08', '2021/08/01', '2021/08/08', '2021/09/19', '2021/12/26', '2022/01/01', '2022/01/16', '2022/01/26', '2022/01/30', '2022/02/20', '2022/02/22', '2022/03/12', '2022/03/13', '2022/03/20', '2022/04/24', '2022/05/03', '2022/05/04', '2022/05/08', '2022/06/20', '2022/07/04', '2022/07/05', '2022/07/24', '2022/08/05', '2022/08/28', '2022/09/04', '2022/09/10', '2022/09/11', '2022/09/25', '2022/10/16', '2022/10/30', '2022/12/18', '2022/12/25', '2023/01/29', '2023/02/10', '2023/02/12', '2023/03/26', '2023/04/02', '2023/04/03', '2023/04/23', '2023/05/07']\nTotal count: 415\nFraction of a Year missing days: 1.1369863013698631\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Revision de dias faltantes en el año",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "news_missing_dates_fomatted = [date.split(\"/\") for date in news_missing_dates]\nnews_missing_dates_fomatted = pd.DataFrame(news_missing_dates_fomatted, columns=[\"year\", \"month\", \"day\"])\nnews_missing_dates_fomatted.groupby([\"year\"]).count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "      month  day\nyear            \n2019    160  160\n2020    165  165\n2021     55   55\n2022     27   27\n2023      8    8\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Revision de dias faltantes por mes en el año",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "news_missing_dates_fomatted.groupby([\"year\", \"month\"]).count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "            day\nyear month     \n2019 01      16\n     02      13\n     03      16\n     04      14\n     05      11\n     06      17\n     07      14\n     08      11\n     09      15\n     10      10\n     11      11\n     12      12\n2020 01      11\n     02      14\n     03       5\n     04       8\n     05       9\n     06      10\n     07      10\n     08      20\n     09      24\n     10      25\n     11      17\n     12      12\n2021 01      27\n     02      14\n     03       6\n     04       3\n     05       1\n     08       2\n     09       1\n     12       1\n2022 01       4\n     02       2\n     03       3\n     04       1\n     05       3\n     06       1\n     07       3\n     08       2\n     09       4\n     10       2\n     12       2\n2023 01       1\n     02       2\n     03       1\n     04       3\n     05       1\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}